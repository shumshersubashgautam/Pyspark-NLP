{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfvpEklI2suElfDwImuoju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shumshersubashgautam/Pyspark-NLP/blob/main/NGRAM_generator_Wordsegmenter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Segmenter-CHINESE**\n",
        "\n",
        "WordSegmenterModel-WSM can tokenize non-english texts. Many languages are not whitespace seperated and their sentences are a concationation of many symbols, like Korean, Japanese or Chinese. Withouth understanding the language splitting the Words into their corrosponding tokens is impossible. \n",
        "The WordSegmenterModel is trained to understand these languages and split then \n",
        "semantically correct.\n",
        "\n"
      ],
      "metadata": {
        "id": "gHxJTMG--WWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "import sparknlp\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "amps8uPZ-wLg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "document_assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "word_segmenter = WordSegmenterModel.pretrained(\"wordseg_gsd_ud_trad\", \"zh\")\\\n",
        "        .setInputCols([\"document\"])\\\n",
        "        .setOutputCol(\"words_segmented\")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[document_assembler, word_segmenter])\n",
        "example = spark.createDataFrame(pd.DataFrame({'text': [\"\"\"然而，這樣的處理也衍生了一些問題。\"\"\"]}))\n",
        "\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "result.select('words_segmented.result').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1QpBRNr-_ts",
        "outputId": "37881da6-408a-4858-c17d-11e2a8cbf069"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordseg_gsd_ud_trad download started this may take some time.\n",
            "Approximate size to download 1.2 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+\n",
            "|                      result|\n",
            "+----------------------------+\n",
            "|[然而, ，, 這樣, 的, 處理...|\n",
            "+----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENGLISH Chunk Extraction with Chunker **"
      ],
      "metadata": {
        "id": "Ga9XSyAi_XEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark==3.3.0  spark-nlp==4.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNlw2M2x_BCd",
        "outputId": "bdc88fb5-f5fb-4913-a04f-dfd0dcbace88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.7/471.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.pipeline import Pipeline\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "Ks03u65Y_naM",
        "outputId": "c40326fa-da97-46ad-8661-0af16cf914ba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark NLP version:  4.4.1\n",
            "Apache Spark version:  3.2.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd4b9bc04f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://99ec2309b1da:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Spark Dataframe**"
      ],
      "metadata": {
        "id": "H741EmVT_xBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O news_category_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test"
      ],
      "metadata": {
        "id": "PKe_MKW3_vb4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark --packages io.delta:delta-core_2.12:1.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQF5_osKAeOQ",
        "outputId": "de455e8c-f3cb-4e57-9f5d-8eb5c6852643"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "io.delta#delta-core_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b8005120-bcd3-4cbf-aad6-3ccee115dd67;1.0\n",
            "\tconfs: [default]\n",
            "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
            "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
            "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
            "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.1.0/delta-core_2.12-1.1.0.jar ...\n",
            "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.1.0!delta-core_2.12.jar (209ms)\n",
            "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
            "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (40ms)\n",
            "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
            "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (37ms)\n",
            ":: resolution report :: resolve 1280ms :: artifacts dl 298ms\n",
            "\t:: modules in use:\n",
            "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
            "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
            "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-b8005120-bcd3-4cbf-aad6-3ccee115dd67\n",
            "\tconfs: [default]\n",
            "\t3 artifacts copied, 0 already retrieved (2841kB/22ms)\n",
            "23/05/05 03:43:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/05/05 03:43:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.0\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.10.11 (main, Apr  5 2023 14:15:10)\n",
            "Spark context Web UI available at http://99ec2309b1da:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1683258224559).\n",
            "SparkSession available as 'spark'.\n",
            ">>> spark\n",
            "<pyspark.sql.session.SparkSession object at 0x7fbb00a205e0>\n",
            ">>> \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 362, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 1447, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1313, in __call__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1283, in _build_args\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1283, in <listcomp>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 363, in signal_handler\n",
            "    raise KeyboardInterrupt()\n",
            "KeyboardInterrupt\n",
            ">>> ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyspark.sql.functions as F\n",
        "\n",
        "# news_df = spark.read\\ .option(\"header\", \"true\")\\ .csv(\"/content/news_category_test.csv\")\\ .withColumnRenamed(\"description\", \"text\")\n",
        "\n",
        "# news_df.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "wz1krnSn_5di",
        "outputId": "478e64e3-3e06-4aa0-fa79-2951af4b5ee5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-567029c86f5a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    news_df = spark.read\\ .option(\"header\", \"true\")\\ .csv(\"/content/news_category_test.csv\")\\ .withColumnRenamed(\"description\", \"text\")\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # applying POS chunker to find a custom pattern\n",
        "\n",
        "# chunker = Chunker() \\\n",
        "#             .setInputCols([\"document\", \"pos\"]) \\\n",
        "#             .setOutputCol(\"chunk\") \\\n",
        "#             .setRegexParsers([\"<NNP>+\", \"<DT>?<JJ>*<NN>\"])\n",
        "\n",
        "# # NNP: Proper Noun\n",
        "# # NN: COmmon Noun\n",
        "# # DT: Determinator (e.g. the)\n",
        "# # JJ: Adjective\n",
        "\n",
        "# chunker.extractParamMap()"
      ],
      "metadata": {
        "id": "3-Fb3TUlAs1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # applying POS chunker to find a custom pattern\n",
        "\n",
        "# chunker = Chunker() \\\n",
        "#             .setInputCols([\"document\", \"pos\"]) \\\n",
        "#             .setOutputCol(\"chunk\") \\\n",
        "#             .setRegexParsers([\"<NNP>+\", \"<DT>?<JJ>*<NN>\"])\n",
        "\n",
        "# # NNP: Proper Noun\n",
        "# # NN: COmmon Noun\n",
        "# # DT: Determinator (e.g. the)\n",
        "# # JJ: Adjective\n",
        "\n",
        "# chunker.extractParamMap()"
      ],
      "metadata": {
        "id": "9tDmuDztALUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BD-trhP1BPBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NGramGenerator**"
      ],
      "metadata": {
        "id": "n-vPhRlOBPOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from pyspark.sql.types import StringType"
      ],
      "metadata": {
        "id": "xxuYO74XBRJ3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLmZVmmnBULU",
        "outputId": "48c4270d-3cc8-49db-b217-b98675f5c9d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark NLP version:  4.4.1\n",
            "Apache Spark version:  3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfTest = spark.createDataFrame([\n",
        "    \"Cloud computing is benefiting major manufacturing companies\",\n",
        "    \"Big data cloud computing cyber security machine learning\"\n",
        "], StringType()).toDF(\"text\")"
      ],
      "metadata": {
        "id": "PoyZJDsaBV6M"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "bigrams = NGramGenerator() \\\n",
        "            .setInputCols([\"token\"]) \\\n",
        "            .setOutputCol(\"bigrams\") \\\n",
        "            .setN(2)\n",
        "\n",
        "trigrams_cum = NGramGenerator() \\\n",
        "            .setInputCols([\"token\"]) \\\n",
        "            .setOutputCol(\"trigrams\") \\\n",
        "            .setN(3)\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    bigrams,\n",
        "    trigrams_cum\n",
        "])"
      ],
      "metadata": {
        "id": "UyC-f-BGBXyE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Pipeline in Spark (DataFrame)\n",
        "model = pipeline.fit(dfTest)\n",
        "prediction = model.transform(dfTest)"
      ],
      "metadata": {
        "id": "F0k1DTWzBaMv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.select(\"bigrams.result\").show(2, truncate=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6yyjbY2BfOC",
        "outputId": "6397c893-49ee-42b5-d292-db0e79d3e9cc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------+\n",
            "|                                                      result|\n",
            "+------------------------------------------------------------+\n",
            "|[Cloud computing, computing is, is benefiting, benefiting...|\n",
            "|[Big data, data cloud, cloud computing, computing cyber, ...|\n",
            "+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.select(\"trigrams.result\").show(2, truncate=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofsTbHHQBg5p",
        "outputId": "b87d2181-f673-4511-ce81-2d4ca0cfa521"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------+\n",
            "|                                                      result|\n",
            "+------------------------------------------------------------+\n",
            "|[Cloud computing is, computing is benefiting, is benefiti...|\n",
            "|[Big data cloud, data cloud computing, cloud computing cy...|\n",
            "+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the Pipeline in Python (string)\n",
        "from sparknlp.base import LightPipeline\n",
        "\n",
        "text = 'Cloud computing is benefiting major manufacturing companies'"
      ],
      "metadata": {
        "id": "P7_xc_5nBitK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = LightPipeline(model).annotate(text)"
      ],
      "metadata": {
        "id": "noRXDWW6BnDQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(result.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk0R2Es_Bowj",
        "outputId": "ef9f7e9c-8ad9-4343-d69e-059f0ed4aaf3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['document', 'token', 'bigrams', 'trigrams']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['bigrams']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpW3GSRUBqfJ",
        "outputId": "eb15ccee-25ad-4928-b463-2bc14971b25b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cloud computing',\n",
              " 'computing is',\n",
              " 'is benefiting',\n",
              " 'benefiting major',\n",
              " 'major manufacturing',\n",
              " 'manufacturing companies']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['trigrams']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYNkzEzjBsDY",
        "outputId": "eb5ca65d-0b87-4a22-fdda-b3bfff477f63"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cloud computing is',\n",
              " 'computing is benefiting',\n",
              " 'is benefiting major',\n",
              " 'benefiting major manufacturing',\n",
              " 'major manufacturing companies']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pSfLeMCBtt8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}